#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”Ÿå­˜åˆ†æSHAPè§£é‡Šæ€§åˆ†æ - Pythonç‰ˆæœ¬
é€‚ç”¨äºXGBoostå’Œéšæœºç”Ÿå­˜æ£®æ—çš„SHAPå€¼è®¡ç®—ä¸å¯è§†åŒ–
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sksurv.metrics import concordance_index_censored
import warnings
import os
from pathlib import Path

# è®¾ç½®å›¾å½¢æ ·å¼å’Œå‚æ•°
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")
warnings.filterwarnings('ignore')

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“å’Œå‚æ•°
plt.rcParams['font.family'] = ['DejaVu Sans', 'SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['figure.facecolor'] = 'white'
plt.rcParams['axes.facecolor'] = 'white'

# åˆ›å»ºè¾“å‡ºç›®å½•
output_dir = Path("Feature_selection")
output_dir.mkdir(exist_ok=True)


def load_and_preprocess_data():
    """
    åŠ è½½å’Œé¢„å¤„ç†ç”Ÿå­˜æ•°æ®
    """
    print("ğŸ”„ æ­£åœ¨åŠ è½½æ•°æ®...")

    # è¯»å–ç”Ÿå­˜æ•°æ®
    try:
        tte_data = pd.read_csv('tte.csv')
        print(f"âœ… ç”Ÿå­˜æ•°æ®åŠ è½½æˆåŠŸ: {tte_data.shape}")
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°tte.csvæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„")
        return None, None

    # è¯»å–ç‰¹å¾å…ƒæ•°æ®
    try:
        metadata = pd.read_excel('fi_lab_metadata.xlsx')
        print(f"âœ… å…ƒæ•°æ®åŠ è½½æˆåŠŸ: {metadata.shape}")

        # ç­›é€‰éœ€è¦çš„ç‰¹å¾å˜é‡ (xvar==1)
        feature_vars = metadata[metadata['xvar'] == 1]['variable_name'].tolist()
        print(f"ğŸ“Š è¯†åˆ«åˆ° {len(feature_vars)} ä¸ªç‰¹å¾å˜é‡")
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°fi_lab_metadata.xlsxæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„")
        return None, None

    return tte_data, feature_vars


def prepare_survival_data(tte_data, feature_vars, endpoint='ICU28D'):
    """
    å‡†å¤‡ç”Ÿå­˜åˆ†ææ•°æ®
    """
    print(f"ğŸ”§ æ­£åœ¨å‡†å¤‡ {endpoint} ç”Ÿå­˜æ•°æ®...")

    # ç­›é€‰æŒ‡å®šendpointçš„æ•°æ®
    survival_data = tte_data[tte_data['testcd'] == endpoint].copy()

    if survival_data.empty:
        print(f"âŒ æœªæ‰¾åˆ° {endpoint} ç›¸å…³æ•°æ®")
        return None, None, None

    # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
    required_cols = ['aval', 'cnsr'] + feature_vars
    missing_cols = [col for col in required_cols if col not in survival_data.columns]

    if missing_cols:
        print(f"âŒ ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
        available_features = [col for col in feature_vars if col in survival_data.columns]
        print(f"ğŸ“‹ å¯ç”¨ç‰¹å¾: {len(available_features)} ä¸ª")
        feature_vars = available_features

    # æå–æ—¶é—´ã€äº‹ä»¶å’Œç‰¹å¾
    time = survival_data['aval'].values
    event = survival_data['cnsr'].values  # 1=event (ä¸å¸¸è§„å®šä¹‰ç›¸å)

    # ç‰¹å¾æ•°æ®
    X = survival_data[feature_vars].copy()

    # å¤„ç†ç¼ºå¤±å€¼
    X = X.fillna(X.median())

    # å¤„ç†åˆ†ç±»å˜é‡
    le_dict = {}
    for col in X.columns:
        if X[col].dtype == 'object':
            le = LabelEncoder()
            X[col] = le.fit_transform(X[col].astype(str))
            le_dict[col] = le

    print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
    print(f"ğŸ“ˆ äº‹ä»¶ç‡: {event.mean():.2%}")

    return X, time, event


def train_xgboost_survival(X, time, event, test_size=0.2):
    """
    è®­ç»ƒXGBoostç”Ÿå­˜æ¨¡å‹
    """
    print("ğŸš€ æ­£åœ¨è®­ç»ƒXGBoostç”Ÿå­˜æ¨¡å‹...")

    # åˆ›å»ºç”Ÿå­˜æ ‡ç­¾ (æ—¶é—´è¶Šé•¿ï¼Œäº‹ä»¶å‘ç”Ÿ=0æ—¶ï¼Œæ ‡ç­¾è¶Šå¤§)
    # ä½¿ç”¨è´Ÿçš„å¯¹æ•°æ—¶é—´ä½œä¸ºå›å½’ç›®æ ‡ï¼Œäº‹ä»¶å‘ç”Ÿæ—¶æƒé‡æ›´é«˜
    y = np.where(event == 1, -np.log(time + 1), np.log(time + 1))

    # æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test, time_train, time_test, event_train, event_test = \
        train_test_split(X, y, time, event, test_size=test_size, random_state=42, stratify=event)

    # è®¡ç®—æ ·æœ¬æƒé‡
    sample_weight = np.where(event_train == 1, 2.0, 1.0)  # ç»™äº‹ä»¶å‘ç”Ÿçš„æ ·æœ¬æ›´é«˜æƒé‡

    # è®­ç»ƒXGBoostæ¨¡å‹
    model = xgb.XGBRegressor(
        n_estimators=100,
        max_depth=5,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        tree_method='hist'
    )

    model.fit(X_train, y_train, sample_weight=sample_weight)

    # è®¡ç®—C-indexè¯„ä¼°æ¨¡å‹æ€§èƒ½
    try:
        pred_train = model.predict(X_train)
        pred_test = model.predict(X_test)

        # è®¡ç®—C-indexï¼ˆéœ€è¦åè½¬é¢„æµ‹å€¼ï¼Œå› ä¸ºæˆ‘ä»¬ç”¨çš„æ˜¯è´Ÿå¯¹æ•°æ—¶é—´ï¼‰
        c_index_train = concordance_index_censored(event_train == 1, time_train, -pred_train)[0]
        c_index_test = concordance_index_censored(event_test == 1, time_test, -pred_test)[0]

        print(f"ğŸ“Š è®­ç»ƒé›† C-index: {c_index_train:.3f}")
        print(f"ğŸ“Š æµ‹è¯•é›† C-index: {c_index_test:.3f}")
    except Exception as e:
        print(f"âš ï¸ C-indexè®¡ç®—å‡ºé”™: {e}")

    return model, X_train, X_test, y_train, y_test


def calculate_shap_values(model, X_train, X_test):
    """
    è®¡ç®—SHAPå€¼
    """
    print("ğŸ” æ­£åœ¨è®¡ç®—SHAPå€¼...")

    # ä½¿ç”¨TreeExplainer
    explainer = shap.TreeExplainer(model)

    # è®¡ç®—æµ‹è¯•é›†çš„SHAPå€¼
    shap_values = explainer.shap_values(X_test)

    print(f"âœ… SHAPå€¼è®¡ç®—å®Œæˆ: {shap_values.shape}")

    return explainer, shap_values


def save_plot_multiple_formats(plt_obj, filename_base, output_dir):
    """
    ä¿å­˜å›¾ç‰‡ä¸ºå¤šç§æ ¼å¼
    """
    formats = ['jpg', 'png', 'svg', 'tif']
    for fmt in formats:
        filepath = output_dir / f"{filename_base}.{fmt}"
        if hasattr(plt_obj, 'savefig'):
            plt_obj.savefig(filepath, format=fmt, bbox_inches='tight', dpi=300, facecolor='white')
        else:
            plt.savefig(filepath, format=fmt, bbox_inches='tight', dpi=300, facecolor='white')
    print(f"ğŸ’¾ å›¾ç‰‡å·²ä¿å­˜: {filename_base}")


def create_shap_visualizations(explainer, shap_values, X_test, endpoint, output_dir):
    """
    åˆ›å»ºSHAPå¯è§†åŒ–å›¾å½¢
    """
    print("ğŸ¨ æ­£åœ¨åˆ›å»ºSHAPå¯è§†åŒ–å›¾å½¢...")

    # 1. å…¨å±€ç‰¹å¾é‡è¦æ€§æ¡å½¢å›¾
    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values, X_test, plot_type="bar", show=False)
    plt.title(f'SHAP Feature Importance - {endpoint}', fontsize=16, fontweight='bold', pad=20)
    plt.xlabel('Mean |SHAP Value|', fontsize=14)
    plt.ylabel('Features', fontsize=14)
    plt.tick_params(axis='both', which='major', labelsize=12)
    plt.tight_layout()
    save_plot_multiple_formats(plt, f'shap_feature_importance_{endpoint}', output_dir)
    plt.close()

    # 2. SHAPæ±‡æ€»å›¾ï¼ˆèœ‚ç¾¤å›¾ï¼‰
    plt.figure(figsize=(12, 10))
    shap.summary_plot(shap_values, X_test, show=False, alpha=0.7)
    plt.title(f'SHAP Summary Plot - {endpoint}', fontsize=16, fontweight='bold', pad=20)
    plt.xlabel('SHAP Value (Impact on Model Output)', fontsize=14)
    plt.ylabel('Features', fontsize=14)
    plt.tick_params(axis='both', which='major', labelsize=12)

    # æ·»åŠ é¢œè‰²æ¡æ ‡ç­¾
    cbar = plt.colorbar(plt.cm.ScalarMappable(cmap=plt.cm.coolwarm), ax=plt.gca())
    cbar.set_label('Feature Value', fontsize=12)
    plt.tight_layout()
    save_plot_multiple_formats(plt, f'shap_summary_plot_{endpoint}', output_dir)
    plt.close()

    # 3. å‰5ä¸ªé‡è¦ç‰¹å¾çš„ä¾èµ–å›¾
    feature_importance = np.abs(shap_values).mean(0)
    top_features_idx = np.argsort(feature_importance)[-5:][::-1]
    top_features = X_test.columns[top_features_idx]

    for i, feature in enumerate(top_features):
        plt.figure(figsize=(10, 6))
        shap.dependence_plot(feature, shap_values, X_test, show=False, alpha=0.7)
        plt.title(f'SHAP Dependence Plot - {feature} ({endpoint})', fontsize=14, fontweight='bold', pad=15)
        plt.xlabel(f'{feature}', fontsize=12)
        plt.ylabel(f'SHAP Value for {feature}', fontsize=12)
        plt.tick_params(axis='both', which='major', labelsize=10)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        save_plot_multiple_formats(plt, f'shap_dependence_{feature}_{endpoint}', output_dir)
        plt.close()

    # 4. å±€éƒ¨è§£é‡Š - ç€‘å¸ƒå›¾ï¼ˆå‰3ä¸ªæ ·æœ¬ï¼‰
    for i in range(min(3, len(X_test))):
        plt.figure(figsize=(12, 8))

        # åˆ›å»ºè§£é‡Šå¯¹è±¡
        explanation = shap.Explanation(
            values=shap_values[i],
            base_values=explainer.expected_value,
            data=X_test.iloc[i].values,
            feature_names=X_test.columns.tolist()
        )

        shap.plots.waterfall(explanation, show=False)
        plt.title(f'SHAP Waterfall Plot - Sample {i + 1} ({endpoint})', fontsize=14, fontweight='bold', pad=15)
        plt.tick_params(axis='both', which='major', labelsize=10)
        plt.tight_layout()
        save_plot_multiple_formats(plt, f'shap_waterfall_sample_{i + 1}_{endpoint}', output_dir)
        plt.close()

    # 5. å†³ç­–å›¾ï¼ˆå‰3ä¸ªæ ·æœ¬ï¼‰
    for i in range(min(3, len(X_test))):
        plt.figure(figsize=(10, 12))
        shap.decision_plot(explainer.expected_value, shap_values[i], X_test.iloc[i],
                           feature_names=X_test.columns.tolist(), show=False)
        plt.title(f'SHAP Decision Plot - Sample {i + 1} ({endpoint})', fontsize=14, fontweight='bold', pad=15)
        plt.tick_params(axis='both', which='major', labelsize=10)
        plt.tight_layout()
        save_plot_multiple_formats(plt, f'shap_decision_sample_{i + 1}_{endpoint}', output_dir)
        plt.close()

    # 6. ç‰¹å¾ç›¸äº’ä½œç”¨çƒ­åŠ›å›¾
    if len(X_test.columns) <= 20:  # åªå¯¹å°‘é‡ç‰¹å¾è®¡ç®—ç›¸äº’ä½œç”¨
        plt.figure(figsize=(12, 10))

        # è®¡ç®—SHAPç›¸äº’ä½œç”¨å€¼ï¼ˆè®¡ç®—é‡å¤§ï¼Œè°¨æ…ä½¿ç”¨ï¼‰
        try:
            interaction_values = explainer.shap_interaction_values(X_test.iloc[:100])  # åªç”¨å‰100ä¸ªæ ·æœ¬
            interaction_mean = np.abs(interaction_values).mean(0)

            # åˆ›å»ºçƒ­åŠ›å›¾
            sns.heatmap(interaction_mean,
                        xticklabels=X_test.columns,
                        yticklabels=X_test.columns,
                        annot=False,
                        cmap='coolwarm',
                        center=0,
                        square=True)
            plt.title(f'SHAP Interaction Values Heatmap - {endpoint}', fontsize=14, fontweight='bold', pad=15)
            plt.xlabel('Features', fontsize=12)
            plt.ylabel('Features', fontsize=12)
            plt.xticks(rotation=45, ha='right')
            plt.yticks(rotation=0)
            plt.tight_layout()
            save_plot_multiple_formats(plt, f'shap_interaction_heatmap_{endpoint}', output_dir)
        except Exception as e:
            print(f"âš ï¸ ç›¸äº’ä½œç”¨å€¼è®¡ç®—å¤±è´¥: {e}")

        plt.close()


def generate_shap_report(shap_values, X_test, endpoint, output_dir):
    """
    ç”ŸæˆSHAPåˆ†ææŠ¥å‘Š
    """
    print("ğŸ“‹ æ­£åœ¨ç”ŸæˆSHAPåˆ†ææŠ¥å‘Š...")

    # è®¡ç®—ç‰¹å¾é‡è¦æ€§
    feature_importance = np.abs(shap_values).mean(0)
    feature_stats = pd.DataFrame({
        'Feature': X_test.columns,
        'Mean_Abs_SHAP': feature_importance,
        'Mean_SHAP': shap_values.mean(0),
        'Std_SHAP': shap_values.std(0),
        'Min_SHAP': shap_values.min(0),
        'Max_SHAP': shap_values.max(0)
    }).sort_values('Mean_Abs_SHAP', ascending=False)

    # ä¿å­˜æŠ¥å‘Š
    report_path = output_dir / f'shap_analysis_report_{endpoint}.csv'
    feature_stats.to_csv(report_path, index=False)

    print(f"ğŸ“Š SHAPåˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    print("\nğŸ“ˆ å‰10ä¸ªæœ€é‡è¦ç‰¹å¾:")
    print(feature_stats.head(10)[['Feature', 'Mean_Abs_SHAP', 'Mean_SHAP']].to_string(index=False))

    return feature_stats


def main():
    """
    ä¸»å‡½æ•°
    """
    print("ğŸ¥ ç”Ÿå­˜åˆ†æSHAPè§£é‡Šæ€§åˆ†æå¼€å§‹\n" + "=" * 50)

    # 1. åŠ è½½æ•°æ®
    tte_data, feature_vars = load_and_preprocess_data()
    if tte_data is None:
        return

    # 2. åˆ†æä¸¤ä¸ªendpoint
    endpoints = ['ICU28D', 'HOSPXXD']

    for endpoint in endpoints:
        print(f"\n{'=' * 20} åˆ†æ {endpoint} {'=' * 20}")

        # å‡†å¤‡æ•°æ®
        X, time, event = prepare_survival_data(tte_data, feature_vars, endpoint)
        if X is None:
            print(f"âš ï¸ è·³è¿‡ {endpoint} åˆ†æ")
            continue

        # è®­ç»ƒæ¨¡å‹
        model, X_train, X_test, y_train, y_test = train_xgboost_survival(X, time, event)

        # è®¡ç®—SHAPå€¼
        explainer, shap_values = calculate_shap_values(model, X_train, X_test)

        # åˆ›å»ºå¯è§†åŒ–
        create_shap_visualizations(explainer, shap_values, X_test, endpoint, output_dir)

        # ç”ŸæˆæŠ¥å‘Š
        feature_stats = generate_shap_report(shap_values, X_test, endpoint, output_dir)

    print(f"\nğŸ‰ åˆ†æå®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° {output_dir} ç›®å½•")
    print("ğŸ“ è¾“å‡ºæ–‡ä»¶åŒ…æ‹¬:")
    print("   - SHAPç‰¹å¾é‡è¦æ€§å›¾")
    print("   - SHAPæ±‡æ€»å›¾")
    print("   - ç‰¹å¾ä¾èµ–å›¾")
    print("   - å±€éƒ¨è§£é‡Šå›¾ï¼ˆç€‘å¸ƒå›¾ã€å†³ç­–å›¾ï¼‰")
    print("   - ç‰¹å¾ç›¸äº’ä½œç”¨çƒ­åŠ›å›¾")
    print("   - åˆ†ææŠ¥å‘ŠCSVæ–‡ä»¶")


if __name__ == "__main__":
    main()
